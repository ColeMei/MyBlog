<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ML on Cole World</title>
    <link>http://localhost:1313/en/tags/ml/</link>
    <description>Recent content in ML on Cole World</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Wed, 02 Apr 2025 13:18:30 +1100</lastBuildDate>
    <atom:link href="http://localhost:1313/en/tags/ml/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>PAC Theory and VC Dimension: The Theoretical Foundations of Machine Learning</title>
      <link>http://localhost:1313/en/posts/pac-theory-and-vc-dimension-the-theoretical-foundations-of-machine-learning/</link>
      <pubDate>Wed, 02 Apr 2025 13:18:30 +1100</pubDate>
      <guid>http://localhost:1313/en/posts/pac-theory-and-vc-dimension-the-theoretical-foundations-of-machine-learning/</guid>
      <description>&lt;aside class=&#34;admonition info&#34;&gt;&#xA;        &lt;div class=&#34;admonition-title&#34;&gt;&#xA;            &lt;div class=&#34;icon&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; width=&#34;24&#34; height=&#34;24&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34; stroke=&#34;currentColor&#34;&#xA;      stroke-width=&#34;2&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; class=&#34;feather feather-info&#34;&gt;&#xA;      &lt;circle cx=&#34;12&#34; cy=&#34;12&#34; r=&#34;10&#34;&gt;&lt;/circle&gt;&#xA;      &lt;line x1=&#34;12&#34; y1=&#34;16&#34; x2=&#34;12&#34; y2=&#34;12&#34;&gt;&lt;/line&gt;&#xA;      &lt;line x1=&#34;12&#34; y1=&#34;8&#34; x2=&#34;12.01&#34; y2=&#34;8&#34;&gt;&lt;/line&gt;&#xA;   &lt;/svg&gt;&lt;/div&gt;&lt;b&gt;This is a info&lt;/b&gt;&#xA;        &lt;/div&gt;&#xA;        &lt;div class=&#34;admonition-content&#34;&gt;This blog is inspired by my experience in COMP90051 Statistical Machine Learning in Unimelb, particularly content from Lectures 6-8. I&amp;rsquo;ve created this summary for my self-learning, organization, and reference purposes.&lt;/div&gt;&#xA;    &lt;/aside&gt;&#xA;&lt;h2 id=&#34;introduction-to-pac-theory&#34;&gt;Introduction to PAC Theory&lt;a href=&#34;#introduction-to-pac-theory&#34; class=&#34;anchor&#34; aria-hidden=&#34;true&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34;&#xA;      stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;&#xA;      &lt;path d=&#34;M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3&#34;&gt;&lt;/path&gt;&#xA;      &lt;line x1=&#34;8&#34; y1=&#34;12&#34; x2=&#34;16&#34; y2=&#34;12&#34;&gt;&lt;/line&gt;&#xA;   &lt;/svg&gt;&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;In machine learning, we often grapple with a fundamental question: how well does our model, trained on limited data, perform on unseen examples? Probably Approximately Correct (PAC) learning theory provides a framework to address this question rigorously.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
